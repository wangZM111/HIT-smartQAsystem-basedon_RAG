import os
import json
import time
import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import tempfile
import embedding_json

# ========== é…ç½®è·¯å¾„ ==========
current_dir = os.path.dirname(os.path.abspath(__file__))
VISITED_PATH = os.path.join(current_dir, "æ•°æ®åº“", "VISITED_PATH.json")
HISTORY_PATH = os.path.join(current_dir, "æ•°æ®åº“", "hit_articles.json")

# ========== ç½‘é¡µè§£æ ==========
def get_urls(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        links = soup.find_all('a', href=True)
        article_links = []
        for link in links:
            href = link['href']
            if href.startswith('/article/'):
                article_links.append(f"ç½‘é¡µç½‘å€") #é¡¹ç›®ä¸å¯¹å¤–æä¾›
        return article_links
    else:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{url}ï¼ŒçŠ¶æ€ç  {response.status_code}")
        return []

def extract_info(page):
    try:
        title = page.text_content('.article-title h3')
        content = page.text_content('.article-content')
        if title and content:
            return title.strip(), content.strip()
    except:
        pass
    return None, None

def all_urls(n=2):
    urls = []
    for i in range(n):
        url_parent = f'å“ˆå·¥å¤§ç½‘é¡µ'
        #æ ¡å†…ä¿¡æ¯ç½‘ç‚¹ï¼Œæœ¬é¡¹ç›®ä¸æä¾›ç›´æ¥åœ°å€
        urls += get_urls(url_parent)
    return urls

# ========== çŠ¶æ€ç®¡ç† ==========
def load_visited_urls():
    """åŠ è½½å·²è®¿é—®çš„ URLã€æ ‡é¢˜å’Œå†…å®¹"""
    if os.path.exists(VISITED_PATH):
        with open(VISITED_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_visited_urls(visited):
    """ä¿å­˜å·²è®¿é—®çš„ URLã€æ ‡é¢˜å’Œå†…å®¹"""
    with open(VISITED_PATH, "w", encoding="utf-8") as f:
        json.dump(visited, f, ensure_ascii=False, indent=2)

def load_old_articles():
    if os.path.exists(HISTORY_PATH):
        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_all_articles(all_articles):
    with open(HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

# ========== ä¸»å‡½æ•°ï¼Œæ”¯æŒä¼ å…¥ page å’Œé¡µæ•° ==========
def run_spider(page, n_pages=2):
    visited_urls = load_visited_urls()
    urls = all_urls(n=n_pages)
    new_urls = [url for url in urls if url not in [entry["url"] for entry in visited_urls]]  # æ£€æŸ¥æ–° URL
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] å…±å‘ç° {len(new_urls)} ç¯‡æ–°æ–‡ç« ")

    data_list = []

    for url in new_urls:
        try:
            page.goto(url)
            page.wait_for_selector('.article-title h3', timeout=10000)
            title, content = extract_info(page)
            if title and content:
                # ä¿å­˜ URLã€æ ‡é¢˜å’Œå†…å®¹
                data_list.append({
                    "url": url,
                    "title": title,
                    "content": content
                })
                visited_urls.append({
                    "url": url,
                    "title": title,
                })
                print(f"âœ… çˆ¬å–æˆåŠŸï¼š{title}")
            else:
                print(f"âš ï¸ å†…å®¹ä¸ºç©ºï¼š{url}")
        except Exception as e:
            print(f"âŒ å‡ºé”™è·³è¿‡ï¼š{url}ï¼ŒåŸå› ï¼š{e}")
            continue

    old_data = load_old_articles()
    combined_data = old_data + data_list
    save_all_articles(combined_data)
    save_visited_urls(visited_urls)  # ä¿å­˜æ›´æ–°åçš„ URLã€æ ‡é¢˜å’Œå†…å®¹

    print(f"\nâœ… æˆåŠŸæ–°å¢ {len(data_list)} ç¯‡æ–‡ç« ï¼Œæ€»è®¡ï¼š{len(combined_data)} ç¯‡\n")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".json", mode="w", encoding="utf-8") as tmp_file:
        json.dump(data_list, tmp_file, ensure_ascii=False, indent=2)
        temp_path = tmp_file.name

    print(f"ğŸ“ æ–°å¢æ–‡ç« å·²ä¿å­˜è‡³ä¸´æ—¶æ–‡ä»¶ï¼š{temp_path}")
    embedding_json.process_file_and_save_information(temp_path, window_size=300, step_size=150,flag=True)

    try:
        os.remove(temp_path)
        print(f"ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤ï¼š{temp_path}")
    except Exception as e:
        print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{e}")


# ========== è°ƒç”¨å…¥å£ ==========
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="HIT æ–°é—»è‡ªåŠ¨çˆ¬è™«")
    parser.add_argument("--pages", type=int, default=2, help="çˆ¬å–å‰å‡ é¡µï¼ˆé»˜è®¤2é¡µï¼‰")
    args = parser.parse_args()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        # ç™»å½•
        page.goto('ç™»å½•é¡µé¢') #é¡¹ç›®ä¸å¯¹å¤–æä¾›
        page.fill('input#username', 'è´¦å·') #é¡¹ç›®ä¸å¯¹å¤–æä¾›
        page.fill('input#password', 'å¯†ç ') #é¡¹ç›®ä¸å¯¹å¤–æä¾›
        page.click('a#login_submit')
        page.wait_for_selector('.article-title h3')

        # è¿è¡Œçˆ¬è™«ï¼ŒæŒ‡å®šé¡µæ•°
        run_spider(page, n_pages=2)

        browser.close()
